# Reports

Generated outputs from model training and evaluation, including visualizations, metrics, and analysis artifacts.

## ⚠️ Reports Not Committed

Most report files are excluded from git due to size or regenerability. This directory provides a **standard location** for output artifacts.

## Generated Artifacts

### Confusion Matrices

**`confusion_matrix_rf.png`** - Random Forest classifier confusion matrix
- **Generated by**: `notebooks/modeling/Final_Random_Forest.ipynb` or `Random_Forest (1).ipynb`
- **Shows**: 15×15 heatmap of predicted vs actual species
- **Resolution**: 150 DPI
- **Format**: PNG with annotations

**How to regenerate**:
```python
# In RF notebook evaluation section
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt="d", cmap="Greens")
plt.savefig("reports/confusion_matrix_rf.png", dpi=150)
```

### Classification Reports

**`rf_report.json`** - Detailed per-class metrics (proposed)
- **Contains**: Precision, recall, F1-score for each of 15 species
- **Format**: JSON with macro/weighted averages

**How to regenerate**:
```python
from sklearn.metrics import classification_report
import json

report = classification_report(y_test, y_pred, output_dict=True)
with open("reports/rf_report.json", "w") as f:
    json.dump(report, f, indent=2)
```

### Model Performance Summaries

Expected outputs:
- `model_comparison.csv` - Accuracy/F1 across different algorithms
- `feature_importance.png` - Top features for Random Forest
- `roc_curves.png` - ROC-AUC curves per class (if generated)
- `training_history.png` - CNN validation accuracy progression

## Metrics Overview

### Random Forest (Current Model)
- **Top-1 Accuracy**: To be measured (target >85%)
- **Top-3 Accuracy**: To be measured (target >95%)
- **Macro F1**: To be calculated
- **Weighted F1**: To be calculated

### CNN Experiments
- **Best Val Accuracy**: 0.5762 (from `modeling_updates.ipynb` logs)
- **Final Snapshot**: Train 0.5714 / Val 0.5714

**Note**: See `POSTER_SUMMARY.md` for detailed verified metrics.

## Regeneration Guide

### Full Evaluation Pipeline

1. **Open evaluation notebook**:
   ```bash
   jupyter notebook notebooks/modeling/Final_Random_Forest.ipynb
   ```

2. **Run evaluation cells** (typically near the end):
   - Import metrics: `from sklearn.metrics import accuracy_score, classification_report, confusion_matrix`
   - Compute predictions: `y_pred = model.predict(X_test)`
   - Generate reports and save to `reports/`

3. **Verify outputs**:
   ```bash
   ls -lh reports/
   ```

### Automated Script (Future)

Once `scripts/evaluate_rf.py` is implemented:
```bash
python scripts/evaluate_rf.py \
  --model services/frog-api/rf_calibrated_model.joblib \
  --scaler services/frog-api/scaler.joblib \
  --test-data data/X_test.npy \
  --output-dir reports/
```

## Directory Structure

```
reports/
├── confusion_matrix_rf.png       # RF confusion matrix
├── rf_report.json                # Per-class metrics
├── feature_importance.png        # Feature rankings
├── model_comparison.csv          # Algorithm comparisons
├── training_logs/                # Detailed training outputs
│   ├── rf_training.log
│   └── cnn_history.json
└── archived/                     # Historical reports
    └── 2025-11-18/
```

## Gitignore Patterns

```gitignore
reports/*.png
reports/*.json
reports/*.csv
reports/training_logs/**
!reports/README.md
```

## Integration with Supabase

### Live Production Metrics

Query real-world performance from deployed model:

```sql
-- Overall stats
SELECT * FROM get_model_stats();

-- Per-species accuracy proxy
SELECT * FROM prediction_accuracy ORDER BY total_predictions DESC;
```

These live metrics complement static evaluation reports and track model drift.

## Best Practices

1. **Timestamp reports** - Include date in filename for historical tracking
2. **Version artifacts** - Tag reports with model version/commit hash
3. **Document parameters** - Save hyperparameters alongside metrics
4. **Compare baselines** - Keep reference metrics for regression detection
5. **Automate regeneration** - Use scripts for reproducibility
